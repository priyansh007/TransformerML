{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f9460df",
   "metadata": {
    "papermill": {
     "duration": 0.007053,
     "end_time": "2023-05-26T01:27:01.684833",
     "exception": false,
     "start_time": "2023-05-26T01:27:01.677780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Encoder\n",
    "\n",
    "Transformer is made of two parts, First is Encoder and Decoder.\n",
    "\n",
    "Encoder contains following components,\n",
    "\n",
    "* Single layer of Neural network: To define word embedding, Similar words are in same neighbourhood\n",
    "            Input: Input array, num of sequence, vocab_len\n",
    "            Output size: same as input\n",
    "* Positional Encoding: Matrix made of meaning of positions\n",
    "            Input: num of sequence, vocab_len\n",
    "            Output size: same as input\n",
    "* Multihead attention: convert previous output to 3 different array of key, query, value. Divide this into n heads, multiply key and query, softmax add with value and get new values.\n",
    "            Input: Prev_output, num of sequence, vocab_len\n",
    "            Output size: num of sequence, vocab_len\n",
    "* Layer Normalization: Add previous output with one before output and add Layer normalization\n",
    "* Add Neural network: Add neuralNet with some hidden layers\n",
    "* Layer Normalization: Add previous output with one before output and add Layer normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1814a4f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:27:01.699627Z",
     "iopub.status.busy": "2023-05-26T01:27:01.698720Z",
     "iopub.status.idle": "2023-05-26T01:27:05.472428Z",
     "shell.execute_reply": "2023-05-26T01:27:05.471305Z"
    },
    "papermill": {
     "duration": 3.783966,
     "end_time": "2023-05-26T01:27:05.475208",
     "exception": false,
     "start_time": "2023-05-26T01:27:01.691242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd763cb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:27:05.490017Z",
     "iopub.status.busy": "2023-05-26T01:27:05.489353Z",
     "iopub.status.idle": "2023-05-26T01:27:05.498295Z",
     "shell.execute_reply": "2023-05-26T01:27:05.497240Z"
    },
    "papermill": {
     "duration": 0.019126,
     "end_time": "2023-05-26T01:27:05.500783",
     "exception": false,
     "start_time": "2023-05-26T01:27:05.481657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, seq_len, d_model):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0,self.d_model,2,dtype=float)\n",
    "        pos = torch.arange(0, self.seq_len, 1, dtype=float).reshape(self.seq_len, 1)\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        odd_PE = torch.sin(pos/denominator)\n",
    "        even_PE = torch.cos(pos/denominator)\n",
    "        final = torch.stack([even_PE,odd_PE],dim=2)\n",
    "        final = torch.flatten(final, start_dim=1, end_dim=2)\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec65fccc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:27:05.515198Z",
     "iopub.status.busy": "2023-05-26T01:27:05.514782Z",
     "iopub.status.idle": "2023-05-26T01:27:05.530837Z",
     "shell.execute_reply": "2023-05-26T01:27:05.529971Z"
    },
    "papermill": {
     "duration": 0.025985,
     "end_time": "2023-05-26T01:27:05.533109",
     "exception": false,
     "start_time": "2023-05-26T01:27:05.507124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scaled_dot_products(q,k,v,mask=None):\n",
    "    #q,k,v each 30 x 8 x 200 x 64\n",
    "    d_k = q.size()[-1] #64\n",
    "    scaled = torch.matmul(q,k.transpose(-1,-2)) / math.sqrt(d_k) # 30 x 8 x 200 x 200\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1) # Matrix of no. seq X no. seq which have self attention and cross attention 30 x 8 x 200 x 200 - Mask seq x seq\n",
    "    values = torch.matmul(attention, v) #30 x 8 x 200 x 64\n",
    "    return values, attention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model #512\n",
    "        self.num_heads = num_heads #8\n",
    "        self.head_dim = d_model // num_heads #64\n",
    "        self.qkvlayer = nn.Linear(d_model, 3*d_model) #512 x 1536\n",
    "        self.linear_layer = nn.Linear(d_model, d_model) #512 x 512\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_length, d_model = x.shape # 30 x 200 x 512\n",
    "        qkv = self.qkvlayer(x) # 30 x 200 x 1536\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim) #30 x 200 x 8 x 192\n",
    "        qkv = qkv.permute(0,2,1,3) #30 x 8 x 200 x 192\n",
    "        q, k ,v = qkv.chunk(3, dim=-1) # each 30 x 8 x 200 x 64\n",
    "        values, attention = scaled_dot_products(q,k,v,mask) #30 x 8 x 200 x 64\n",
    "        values = values.reshape(batch_size, seq_length, self.num_heads*self.head_dim) #30 x 200 x 512\n",
    "        out = self.linear_layer(values) #30 x 200 x 512\n",
    "        return values\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameter_shape = parameters_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(parameters_shape))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i+1) for i in range(len(self.parameter_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        var = ((inputs-mean) ** 2).mean(dim=dims, keepdim=True) #30x200x1\n",
    "        std = (var+self.eps).sqrt()\n",
    "        y = (inputs-mean)/std\n",
    "        return self.gamma * y + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "199d361f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:27:05.548337Z",
     "iopub.status.busy": "2023-05-26T01:27:05.547580Z",
     "iopub.status.idle": "2023-05-26T01:27:05.555416Z",
     "shell.execute_reply": "2023-05-26T01:27:05.554451Z"
    },
    "papermill": {
     "duration": 0.018555,
     "end_time": "2023-05-26T01:27:05.557841",
     "exception": false,
     "start_time": "2023-05-26T01:27:05.539286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim, dropout_rate=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81afe9a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:27:05.571918Z",
     "iopub.status.busy": "2023-05-26T01:27:05.571562Z",
     "iopub.status.idle": "2023-05-26T01:27:05.579678Z",
     "shell.execute_reply": "2023-05-26T01:27:05.578478Z"
    },
    "papermill": {
     "duration": 0.017858,
     "end_time": "2023-05-26T01:27:05.581950",
     "exception": false,
     "start_time": "2023-05-26T01:27:05.564092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        self.ffn = FeedForward(d_model, ffn_hidden, drop_prob)\n",
    "    def forward(self, x):\n",
    "        resid_x = x\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm(resid_x+x)\n",
    "        resid_x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm(x+resid_x)\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1565163a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:27:05.597902Z",
     "iopub.status.busy": "2023-05-26T01:27:05.597104Z",
     "iopub.status.idle": "2023-05-26T01:27:05.604602Z",
     "shell.execute_reply": "2023-05-26T01:27:05.603430Z"
    },
    "papermill": {
     "duration": 0.017654,
     "end_time": "2023-05-26T01:27:05.607292",
     "exception": false,
     "start_time": "2023-05-26T01:27:05.589638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers_encoder):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
    "                                     for _ in range(num_layers_encoder)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1040103f",
   "metadata": {
    "papermill": {
     "duration": 0.005874,
     "end_time": "2023-05-26T01:27:05.619435",
     "exception": false,
     "start_time": "2023-05-26T01:27:05.613561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586d9f5",
   "metadata": {
    "papermill": {
     "duration": 0.006113,
     "end_time": "2023-05-26T01:27:05.631791",
     "exception": false,
     "start_time": "2023-05-26T01:27:05.625678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Decoder contains following components,\n",
    "\n",
    "* Single layer of Neural network: To define word embedding, Similar words are in same neighbourhood\n",
    "* Positional Encoding: Matrix made of meaning of positions\n",
    "* Multihead attention1 with masking: convert previous output to 3 different array of key, query, value. Divide this into n heads, multiply key and query, softmax add with value and get new values.\n",
    "* Layer Normalization + Dropout: Add previous output with one before output and add Layer normalization\n",
    "* Multihead attention2 without masking: Take Query from previous output and Key and Value from Encoder to this Multihead attention unit\n",
    "* Layer Normalization + Dropout: Add previous output with one before output and add Layer normalization\n",
    "* Add Neural network: Add neuralNet with some hidden layers\n",
    "* Layer Normalization + Dropout: Add previous output with one before output and add Layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "486e6f78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:27:05.645898Z",
     "iopub.status.busy": "2023-05-26T01:27:05.645473Z",
     "iopub.status.idle": "2023-05-26T01:27:05.656011Z",
     "shell.execute_reply": "2023-05-26T01:27:05.654867Z"
    },
    "papermill": {
     "duration": 0.020422,
     "end_time": "2023-05-26T01:27:05.658278",
     "exception": false,
     "start_time": "2023-05-26T01:27:05.637856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionSecond(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model #512\n",
    "        self.num_heads = num_heads #8\n",
    "        self.head_dim = d_model // num_heads #64\n",
    "        self.q_layer = nn.Linear(d_model, d_model)\n",
    "        self.kvlayer = nn.Linear(d_model, 2*d_model) #512 x 1024\n",
    "        self.linear_layer = nn.Linear(d_model, d_model) #512 x 512\n",
    "    \n",
    "    def forward(self, x, encoder_output):\n",
    "        batch_size, seq_length, d_model = x.shape # 30 x 200 x 512\n",
    "        kv = self.kvlayer(encoder_output) # 30 x 200 x 1536\n",
    "        q = self.q_layer(x)\n",
    "        kv = kv.reshape(batch_size, seq_length, self.num_heads, 2*self.head_dim) #30 x 200 x 8 x 128\n",
    "        q = q.reshape(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        kv = kv.permute(0,2,1,3) #30 x 8 x 200 x 128\n",
    "        q = q.permute(0,2,1,3)\n",
    "        k ,v = kv.chunk(2, dim=-1) # each 30 x 8 x 200 x 64\n",
    "        values, attention = scaled_dot_products(q,k,v,None) #30 x 8 x 200 x 64\n",
    "        values = values.reshape(batch_size, seq_length, self.num_heads*self.head_dim) #30 x 200 x 512\n",
    "        out = self.linear_layer(values) #30 x 200 x 512\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97fe00ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:27:05.673147Z",
     "iopub.status.busy": "2023-05-26T01:27:05.672243Z",
     "iopub.status.idle": "2023-05-26T01:27:05.681888Z",
     "shell.execute_reply": "2023-05-26T01:27:05.680767Z"
    },
    "papermill": {
     "duration": 0.019784,
     "end_time": "2023-05-26T01:27:05.684582",
     "exception": false,
     "start_time": "2023-05-26T01:27:05.664798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.attention2 = MultiHeadAttentionSecond(d_model, num_heads)\n",
    "        self.norm = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        self.ffn = FeedForward(d_model, ffn_hidden, drop_prob)\n",
    "    def forward(self, x, encoder_output, decoder_mask):\n",
    "        resid_x = x\n",
    "        x = self.attention(x, decoder_mask)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm(resid_x+x)\n",
    "        resid_x = x\n",
    "        x = self.attention2(x, encoder_output)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm(resid_x+x)\n",
    "        resid_x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm(x+resid_x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92dbb8dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:27:05.698592Z",
     "iopub.status.busy": "2023-05-26T01:27:05.698183Z",
     "iopub.status.idle": "2023-05-26T01:27:05.707103Z",
     "shell.execute_reply": "2023-05-26T01:27:05.705975Z"
    },
    "papermill": {
     "duration": 0.018746,
     "end_time": "2023-05-26T01:27:05.709548",
     "exception": false,
     "start_time": "2023-05-26T01:27:05.690802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SequentialDecoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x,encoder_output,mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            x = module(x,encoder_output,mask)\n",
    "        return x\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers_decoder):\n",
    "        super().__init__()\n",
    "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
    "                                     for _ in range(num_layers_decoder)])\n",
    "        \n",
    "    def forward(self, x, encoder_output, mask):\n",
    "        x = self.layers(x, encoder_output, mask)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c5b00",
   "metadata": {
    "papermill": {
     "duration": 0.005827,
     "end_time": "2023-05-26T01:27:05.721606",
     "exception": false,
     "start_time": "2023-05-26T01:27:05.715779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a003f403",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:27:05.735830Z",
     "iopub.status.busy": "2023-05-26T01:27:05.735178Z",
     "iopub.status.idle": "2023-05-26T01:27:05.740720Z",
     "shell.execute_reply": "2023-05-26T01:27:05.739797Z"
    },
    "papermill": {
     "duration": 0.015503,
     "end_time": "2023-05-26T01:27:05.743148",
     "exception": false,
     "start_time": "2023-05-26T01:27:05.727645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "max_seq_len = 200\n",
    "batch_size = 30\n",
    "\n",
    "num_layers_encoder=5\n",
    "num_layers_decoder=6\n",
    "\n",
    "ffn_hidden_layers = 2048\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00eb3a77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:27:05.758020Z",
     "iopub.status.busy": "2023-05-26T01:27:05.757360Z",
     "iopub.status.idle": "2023-05-26T01:27:16.552835Z",
     "shell.execute_reply": "2023-05-26T01:27:16.551963Z"
    },
    "papermill": {
     "duration": 10.806014,
     "end_time": "2023-05-26T01:27:16.555386",
     "exception": false,
     "start_time": "2023-05-26T01:27:05.749372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor_English = torch.randn((batch_size, max_seq_len, d_model)).float()\n",
    "tensor_Gujarati = torch.randn((batch_size, max_seq_len, d_model)).float()\n",
    "mask = torch.full([max_seq_len, max_seq_len] , float('-inf'))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "encoder = Encoder(d_model, ffn_hidden_layers, num_heads, dropout, num_layers_encoder)\n",
    "decoder = Decoder(d_model, ffn_hidden_layers, num_heads, dropout, num_layers_decoder)\n",
    "single_layer = nn.Linear(d_model,d_model)\n",
    "positional_encoding = PositionalEncoding(max_seq_len,d_model)\n",
    "\n",
    "encoder_positional = positional_encoding.forward()\n",
    "input_Encoder = single_layer(tensor_English) \n",
    "output_encoder = encoder.forward(input_Encoder+encoder_positional.float())\n",
    "\n",
    "input_Decoder = single_layer(tensor_Gujarati)\n",
    "output_Decoder = decoder.forward((input_Decoder+encoder_positional.float()), output_encoder, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55dd1f8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:27:16.570029Z",
     "iopub.status.busy": "2023-05-26T01:27:16.569300Z",
     "iopub.status.idle": "2023-05-26T01:27:16.577184Z",
     "shell.execute_reply": "2023-05-26T01:27:16.576115Z"
    },
    "papermill": {
     "duration": 0.018013,
     "end_time": "2023-05-26T01:27:16.579590",
     "exception": false,
     "start_time": "2023-05-26T01:27:16.561577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 200, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_encoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55272541",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T01:27:16.594491Z",
     "iopub.status.busy": "2023-05-26T01:27:16.594085Z",
     "iopub.status.idle": "2023-05-26T01:27:16.600380Z",
     "shell.execute_reply": "2023-05-26T01:27:16.599323Z"
    },
    "papermill": {
     "duration": 0.017057,
     "end_time": "2023-05-26T01:27:16.603019",
     "exception": false,
     "start_time": "2023-05-26T01:27:16.585962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 200, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_Decoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e494bc54",
   "metadata": {
    "papermill": {
     "duration": 0.006141,
     "end_time": "2023-05-26T01:27:16.615684",
     "exception": false,
     "start_time": "2023-05-26T01:27:16.609543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29.429294,
   "end_time": "2023-05-26T01:27:18.446722",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-26T01:26:49.017428",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
